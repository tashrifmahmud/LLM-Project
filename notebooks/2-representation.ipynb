{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zi53wUtaSMt",
        "outputId": "54b405a6-9ca2-43f9-ccae-380753abaad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HakgHZ8BaaBm",
        "outputId": "a1c0e8f4-42b6-421d-8415-518848c679f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# File path\n",
        "file_path = '/content/drive/MyDrive/1_Data/ds_clean.pkl'\n",
        "\n",
        "# Load the dataset\n",
        "with open(file_path, 'rb') as f:\n",
        "    ds_clean = pickle.load(f)\n",
        "\n",
        "# Check the dataset\n",
        "print(ds_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifmorJ2babMu",
        "outputId": "44179fa2-eef2-4824-fe88-5b700c206561"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 44972\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 5622\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['document', 'summary'],\n",
            "        num_rows: 5622\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a lot of research and reading I decided to use tensorflow based encoder-decoder for to build my document summarization model."
      ],
      "metadata": {
        "id": "ALSKz8dXfD_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be following instructions for this following process."
      ],
      "metadata": {
        "id": "yFrwSmirH4co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "hKIUTUC4bM9g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Tokenize the text:**"
      ],
      "metadata": {
        "id": "iD46-0V00GnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers for documents and summaries."
      ],
      "metadata": {
        "id": "dNs76syRs_yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")  # Limit to top 10,000 words"
      ],
      "metadata": {
        "id": "RWbp9qY8-0lF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit tokenizer on both documents and summaries of train data\n",
        "all_texts = ds_clean['train']['document'] + ds_clean['train']['summary']\n",
        "tokenizer.fit_on_texts(all_texts)"
      ],
      "metadata": {
        "id": "oq8ci85b_FxL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the documents and summaries\n",
        "X_train_docs = tokenizer.texts_to_sequences(ds_clean['train']['document'])\n",
        "y_train_summaries = tokenizer.texts_to_sequences(ds_clean['train']['summary'])\n",
        "X_val_docs = tokenizer.texts_to_sequences(ds_clean['validation']['document'])\n",
        "y_val_summaries = tokenizer.texts_to_sequences(ds_clean['validation']['summary'])"
      ],
      "metadata": {
        "id": "G3Hlm9u5_F6x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can convert documents and summaries to sequence as per instructions."
      ],
      "metadata": {
        "id": "ucc0MDLmwXpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the firs tokenized documents and summary."
      ],
      "metadata": {
        "id": "yl50qEVPuCNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenized document:\", X_train_docs[0])\n",
        "print(\"Tokenized summary:\", y_train_summaries[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XS-SQLwxGM7",
        "outputId": "5c2d1e61-6337-4617-c94c-c679eed57abc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized document: [158, 1, 1099, 181, 8, 68, 335, 3294, 181, 2, 80, 224, 4, 2, 341, 49, 9, 46, 1, 900, 2, 2444, 4, 682, 431, 213, 6, 699, 431, 26, 54, 1, 10, 46, 407, 959, 5, 2483, 1439, 10, 2, 57, 3591, 772, 9, 689, 3060, 2, 1, 18, 1, 250, 52, 167, 68, 1922, 46, 4, 2, 100, 471, 1, 10, 94, 2, 1198, 1, 103, 2, 872, 341, 4329, 26, 9, 1, 52, 959, 3, 22, 894, 108, 3, 1, 1, 21, 1, 1444, 1, 1213, 3, 1, 959, 318, 7, 928, 2, 3943, 954, 11, 506, 3, 873, 1, 18, 1, 192, 18, 1, 1, 3747, 22, 1922, 2679, 1, 93, 6, 47, 2, 291, 1, 2, 1, 545, 522, 3, 1, 2939, 739, 2, 759, 1376, 6, 99, 1332, 192, 254, 331, 44, 639, 55, 3, 771, 57, 10, 303, 3315, 2, 177, 4853, 1430, 119, 1, 10, 4691, 89, 341, 5, 1, 8, 2, 57, 1198, 91, 28, 22, 1210, 1862, 420, 3, 1, 1, 524, 1492, 2, 3943, 954, 1, 27, 1620, 88, 41, 534, 1325, 508, 9, 150, 2, 1986, 220, 69, 2, 1198, 318, 1, 959, 7, 377, 131, 20, 41, 60, 1, 7, 249, 4, 2, 872, 130, 271, 2, 3943, 954, 1216, 3, 1, 206, 2, 4199, 132, 689, 984, 2, 954, 1590, 88, 2604, 51, 3930, 9, 959, 2, 334, 3943, 1, 95, 1215, 109, 1327, 150, 2, 1198, 29, 318, 1, 959, 132, 814, 2, 290, 240, 271, 4, 4691, 7, 67, 79, 27, 253, 3901, 1221, 2134, 1, 29, 1, 8, 2, 695, 4691, 4416, 11, 2339, 3, 403, 241, 41, 2556, 1116]\n",
            "Tokenized summary: [86, 2, 3943, 954, 1590, 3, 1, 89, 341, 27, 2, 1198, 95, 318, 1, 959, 49, 1, 52, 959, 36, 40, 3495, 108, 3, 3729, 959, 178, 2007, 10, 2, 682, 431, 1, 1, 1846, 12, 3637, 1, 670, 264, 2, 3943, 954, 285, 11, 375, 97, 16, 36, 40, 506, 3, 873, 1, 18, 1, 27, 2, 304, 502, 8, 2, 1, 11, 1620, 643, 3, 41, 534, 1133, 55, 10, 1327, 3591]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Padding and Truncating:**"
      ],
      "metadata": {
        "id": "XKQgxcDg0b9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the sequences generated by tokenizers vary in length, we need to pad them to a fixed length to esnure the same shape for training the model."
      ],
      "metadata": {
        "id": "rrEhSOzNxWUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set max length for padding\n",
        "max_doc_length = 100\n",
        "max_summary_length = 100"
      ],
      "metadata": {
        "id": "nIgj5do4_oGA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences\n",
        "X_train_docs_padded = pad_sequences(X_train_docs, maxlen=max_doc_length, padding='post', truncating='post')\n",
        "y_train_summaries_padded = pad_sequences(y_train_summaries, maxlen=max_summary_length, padding='post', truncating='post')\n",
        "X_val_docs_padded = pad_sequences(X_val_docs, maxlen=max_doc_length, padding='post', truncating='post')\n",
        "y_val_summaries_padded = pad_sequences(y_val_summaries, maxlen=max_summary_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "ejt4gaJT_o06"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape\n",
        "print(\"Padded X_train shape:\", X_train_docs_padded.shape)\n",
        "print(\"Padded y_train shape:\", y_train_summaries_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7UvtBFozzls",
        "outputId": "78652f17-e625-4671-b11d-a8d5825cf578"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded X_train shape: (44972, 100)\n",
            "Padded y_train shape: (44972, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build the Model:**"
      ],
      "metadata": {
        "id": "AVpU5J4CghhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we'll create a simple encoder-decoder model using TensorFlow's Sequential API. We'll use the LSTM (Long Short-Term Memory) layers for both encoding and decoding."
      ],
      "metadata": {
        "id": "ckrt7Pyi0uxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed"
      ],
      "metadata": {
        "id": "XLACBFPn0p-m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "embedding_dim = 32  # Size of embedding vector\n",
        "latent_dim = 32  # Latent dimensionality of the encoding space"
      ],
      "metadata": {
        "id": "HL-CrRIT1IHv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_doc_length,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)"
      ],
      "metadata": {
        "id": "0wqPCeRk10Xe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_summary_length,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "Vy9uHKwQ51NZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "pLY6VgQpBLjd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hF6DT4t8BNcS",
        "outputId": "4a26aa79-6078-4725-ee28-f7bfb6d48d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m26,599,744\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m26,599,744\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │          \u001b[38;5;34m8,320\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│                           │ \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)]       │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m),      │          \u001b[38;5;34m8,320\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
              "│                           │ \u001b[38;5;34m32\u001b[0m)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m831242\u001b[0m)    │     \u001b[38;5;34m27,430,986\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,599,744</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,599,744</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]       │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">831242</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,430,986</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,647,114\u001b[0m (307.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,647,114</span> (307.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,647,114\u001b[0m (307.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,647,114</span> (307.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Training the Model:**"
      ],
      "metadata": {
        "id": "rH00uN6jJfhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train the model, for training we will use teacher forcing algorithm which is common in sequence to sequence model tasks like summarization. We need to shift the target summaries to the right (the target sequence at time step t is the summary at time step t+1 during training)."
      ],
      "metadata": {
        "id": "YjAJd37xIVUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "THLSWqekBPPz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift the target sequence by one timestep for teacher forcing\n",
        "y_train_summaries_shifted = np.zeros_like(y_train_summaries_padded)\n",
        "y_train_summaries_shifted[:, :-1] = y_train_summaries_padded[:, 1:]"
      ],
      "metadata": {
        "id": "pEhsEdqZBYNr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_summaries_shifted = np.zeros_like(y_val_summaries_padded)\n",
        "y_val_summaries_shifted[:, :-1] = y_val_summaries_padded[:, 1:]"
      ],
      "metadata": {
        "id": "XhV-MNvDBqF5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit([X_train_docs_padded, y_train_summaries_padded],\n",
        "          np.expand_dims(y_train_summaries_shifted, -1),\n",
        "          epochs=5, batch_size=16, validation_data=([X_val_docs_padded, y_val_summaries_padded], np.expand_dims(y_val_summaries_shifted, -1)))"
      ],
      "metadata": {
        "id": "N8bTbP0RBbwy",
        "outputId": "fb1d0d69-f85e-43c9-9b91-64cbe7ef4564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2811/2811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1174s\u001b[0m 414ms/step - loss: 7.1245 - val_loss: 5.9317\n",
            "Epoch 2/5\n",
            "\u001b[1m2811/2811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1163s\u001b[0m 414ms/step - loss: 5.8407 - val_loss: 5.5110\n",
            "Epoch 3/5\n",
            "\u001b[1m2811/2811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 414ms/step - loss: 5.4382 - val_loss: 5.2808\n",
            "Epoch 4/5\n",
            "\u001b[1m2811/2811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 414ms/step - loss: 5.2500 - val_loss: 5.1785\n",
            "Epoch 5/5\n",
            "\u001b[1m2811/2811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 414ms/step - loss: 5.1386 - val_loss: 5.0809\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e805cd6c070>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('summary_model_v1.keras')"
      ],
      "metadata": {
        "id": "YmlciK-aBeUv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Model Evaluation:**"
      ],
      "metadata": {
        "id": "bipDAuPkebBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Reload the saved model\n",
        "model = load_model('/content/drive/MyDrive/1_Data/summary_model_v1.keras')"
      ],
      "metadata": {
        "id": "rRATtJUDh3Rk",
        "outputId": "84ed8ee0-5600-481e-acaf-fe2aaa7830f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File not found: filepath=/content/drive/MyDrive/1_Data/summary_model_v1.keras. Please ensure the file is an accessible `.keras` zip file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cbd2af80c644>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Reload the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/1_Data/summary_model_v1.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;34mf\"File not found: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;34m\"Please ensure the file is an accessible `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File not found: filepath=/content/drive/MyDrive/1_Data/summary_model_v1.keras. Please ensure the file is an accessible `.keras` zip file."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the test data now."
      ],
      "metadata": {
        "id": "LT3hMw7Ofeyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the test data\n",
        "X_test_docs = tokenizer.texts_to_sequences(ds_clean['test']['document'])\n",
        "y_test_summaries = tokenizer.texts_to_sequences(ds_clean['test']['summary'])"
      ],
      "metadata": {
        "id": "KO8HZQ_3ddsg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences for the test data\n",
        "X_test_docs_padded = pad_sequences(X_test_docs, maxlen=max_doc_length, padding='post', truncating='post')\n",
        "y_test_summaries_padded = pad_sequences(y_test_summaries, maxlen=max_summary_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "cYIJny16g3m-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_summaries_shifted = np.roll(y_test_summaries_padded, shift=1, axis=1)"
      ],
      "metadata": {
        "id": "pT_HC8-0hVF1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.evaluate([X_test_docs_padded, y_test_summaries_padded],\n",
        "                           np.expand_dims(y_test_summaries_shifted, -1))\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "rQAyGUxBetBN",
        "outputId": "dc10e7cd-520f-44f2-b393-9be52d161cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b68f185d9d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_loss = model.evaluate([X_test_docs_padded, y_test_summaries_padded], \n\u001b[0m\u001b[1;32m      2\u001b[0m                            np.expand_dims(y_test_summaries_shifted, -1))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ca6iwfOYhJL3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}